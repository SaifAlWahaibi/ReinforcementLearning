# ReinforcementLearning
Different reinforcement learning algorithm

Q-Learning Introduction: -

![QL_S1](https://github.com/SaifAlWahaibi/ReinforcementLearning/assets/106843163/1848529f-d46c-43fd-a3c3-0a36a040965e)

![QL_S2](https://github.com/SaifAlWahaibi/ReinforcementLearning/assets/106843163/31ae0a25-cd0b-45c0-8646-0bb20dabee0e)

![QL_S3](https://github.com/SaifAlWahaibi/ReinforcementLearning/assets/106843163/1c02fbe0-a5bd-40d4-ad2c-d91107ff1b14)

	Initialize ğ‘„Â Ì‚_ğœ½ (ğ‘ ,ğ‘) with random weights
	for episode = 1, 2, 3, â€¦, E do
		Initialize environment ğ‘ _ğŸ
		for t = 0, 1, 2, â€¦, T do
			 Select action ğ‘_ğ’• randomly with probability ğœ–, otherwise
 				ğ‘_ğ’•=argmaxâ”¬(ğ‘_ğ’• )â¡ã€–ğ‘„Â Ì‚_ğœ½ (ğ‘ _ğ’•,ğ‘_ğ’• )ã€—
			Execute action ğ‘_ğ’• in environment and observe ğ‘Ÿ_(ğ’•+ğŸ), ğ‘ _(ğ’•+ğŸ), 				and terminal or truncate flags
			Set TD target ğ›¿_ğ‘‡ğ·=ğ‘Ÿ_(ğ’•+ğŸ) if terminal or truncate, otherwise 				ğ›¿_ğ‘‡ğ·=ğ‘Ÿ_(ğ’•+ğŸ)+ğ›¾  maxâ”¬(ğ‘_(ğ’•+ğŸ) )â¡ã€–ğ‘„Â Ì‚_ğœ½ (ğ‘ _(ğ’•+ğŸ),ğ‘_(ğ’•+ğŸ) )ã€—
		Perform a gradient descent step on 
			ğ’¥(ğœ½)=ğ”¼_ğœ‹ [(ğ›¿_ğ‘‡ğ·âˆ’ğ‘„Â Ì‚_ğœ½ (ğ‘ _ğ’•,ğ‘_ğ’• ))^2 ]
		Set ğ‘ _(ğ’•+ğŸ) as current state
	end for
end for
